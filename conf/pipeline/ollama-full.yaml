# conf/pipeline/ollama-full.yaml

# =================================================================
# Full Ollama 파이프라인 모델 설정 (Full Ollama Pipeline Model Configurations)
# =================================================================
# This configuration aims to use Ollama for everything possible
# Note: Requires code modifications for full Ollama support

# RAG 파이프라인의 Tool Calling 및 질의 재구성에 사용할 모델
# ⚠️  EXPERIMENTAL: Requires custom Ollama function calling implementation
tool_calling_model: "qwen2:7b"        # Would need custom function calling
rewriter_model: "qwen2:7b"            # Can use Ollama with custom prompts
query_rewriter_type: "ollama"         # Would need OllamaQueryRewriter implementation

# 최종 답변 생성에 사용할 모델 유형 및 이름
generator_type: "ollama"
generator_model_name: "qwen2:7b"      # Ollama for answer generation

# 검색 도구(scientific_search)에서 반환할 문서의 기본 개수
default_top_k: 5

# =================================================================
# Implementation Requirements for Full Ollama Support:
# =================================================================
# 1. Ollama Function Calling:
#    - Implement custom JSON parsing for tool calls
#    - Add structured output prompting to Ollama models
#    - Handle tool parameter extraction manually
#
# 2. Ollama Query Rewriting:
#    - Use OllamaQueryRewriter with custom prompts
#    - Implement response parsing and cleaning
#    - Handle fallback cases gracefully
#
# 3. Model Selection Considerations:
#    - qwen2:7b: Good for general tasks, fast inference
#    - llama3.1:8b: Better reasoning, slower but more accurate
#    - Consider model size vs performance trade-offs
# =================================================================
