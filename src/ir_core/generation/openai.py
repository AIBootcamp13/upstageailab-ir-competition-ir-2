# src/ir_core/generation/openai.py

import os
from typing import List, Optional, Any, cast, Dict
import openai
import jinja2
from .base import BaseGenerator
from ..analysis.query_analyzer import QueryAnalyzer

class OpenAIGenerator(BaseGenerator):
    """
    OpenAI ëª¨ë¸ì„ ìœ„í•œ BaseGeneratorì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.
    ì´ í´ë˜ìŠ¤ëŠ” OpenAI APIì™€ ìƒí˜¸ ì‘ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ë©°,
    Jinja2 í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    def __init__(
        self,
        model_name: str,
        prompt_template_path: str,
        persona_path: Optional[str] = None,
        client: Optional[openai.OpenAI] = None,
        use_prompt_pipeline: bool = False,
        prompt_pipeline_config: Optional[Dict[str, Any]] = None,
    ):
        """
        OpenAI ìƒì„±ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_name (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸ì˜ ì´ë¦„.
            prompt_template_path (str): ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  Jinja2 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë””ë ‰í† ë¦¬.
            persona_path (str, optional): ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì‚¬ìš©í•  í˜ë¥´ì†Œë‚˜ íŒŒì¼ ê²½ë¡œ. Defaults to None.
            client (Optional[openai.OpenAI], optional): ë¯¸ë¦¬ ì„¤ì •ëœ OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤. Defaults to None.
            use_prompt_pipeline (bool): í”„ë¡¬í”„íŠ¸ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì—¬ë¶€. Defaults to False.
            prompt_pipeline_config (Optional[Dict[str, Any]]): í”„ë¡¬í”„íŠ¸ íŒŒì´í”„ë¼ì¸ ì„¤ì •. Defaults to None.
        """
        self.model_name = model_name
        self.client = client or openai.OpenAI()
        self.prompt_template_path = prompt_template_path
        self.use_prompt_pipeline = use_prompt_pipeline
        self.prompt_pipeline_config = prompt_pipeline_config or {}

        # Load centralized settings for prompt pipeline if not provided
        if not self.prompt_pipeline_config and self.use_prompt_pipeline:
            from ..config import settings
            self.prompt_pipeline_config = getattr(settings, 'prompt_pipeline_config', {})

        # Initialize QueryAnalyzer for centralized query classification
        self.query_analyzer = QueryAnalyzer()

        # Jinja2 í™˜ê²½ì„ ì„¤ì •í•˜ì—¬ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ í…œí”Œë¦¿ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        self.jinja_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(os.getcwd()),
            trim_blocks=True,
            lstrip_blocks=True,
        )

                # --- í˜ë¥´ì†Œë‚˜ ë¡œë”© ë¡œì§ (ìˆ˜ì •ë¨) ---
        # ì´ì œ ëª…ì‹œì ìœ¼ë¡œ ì œê³µëœ persona_pathì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        self.default_persona = ""
        if persona_path:
            try:
                with open(persona_path, "r", encoding="utf-8") as f:
                    self.default_persona = f.read().strip()
                print(f"âœ… í˜ë¥´ì†Œë‚˜ íŒŒì¼ '{persona_path}'ì—ì„œ í˜ë¥´ì†Œë‚˜ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except FileNotFoundError:
                print(f"âš ï¸  í˜ë¥´ì†Œë‚˜ íŒŒì¼ '{persona_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        # Load max_tokens from settings
        from ..config import settings
        self.max_tokens = getattr(settings, 'GENERATOR_MAX_TOKENS', 1024)


    def _render_prompt(self, query: str, context_docs: List[str], template_path: str) -> str:
        """ì§€ì •ëœ ê²½ë¡œì˜ Jinja2 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¡œë“œí•˜ê³  ë Œë”ë§í•©ë‹ˆë‹¤."""
        try:
            template = self.jinja_env.get_template(template_path)
            return template.render(query=query, context_docs=context_docs)
        except jinja2.TemplateNotFound:
            raise FileNotFoundError(
                f"'{template_path}'ì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                f"í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”."
            )

    def generate(
        self,
        query: str,
        context_docs: List[str],
        prompt_template_path: Optional[str] = None,
    ) -> str:
        """
        í…œí”Œë¦¿í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI Chat Completions APIë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        í”„ë¡¬í”„íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ì§€ì›í•©ë‹ˆë‹¤.
        """
        # í˜¸ì¶œ ì‹œ íŠ¹ì • í…œí”Œë¦¿ ê²½ë¡œê°€ ì œê³µë˜ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ê³ , ì•„ë‹ˆë©´ ì¸ìŠ¤í„´ìŠ¤ì˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        template_to_use = prompt_template_path or self.prompt_template_path

        # í”„ë¡¬í”„íŠ¸ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì‹œ
        if self.use_prompt_pipeline:
            return self._generate_with_pipeline(query, context_docs, template_to_use)
        else:
            return self._generate_single_prompt(query, context_docs, template_to_use)

    def _generate_single_prompt(
        self,
        query: str,
        context_docs: List[str],
        template_path: str,
    ) -> str:
        """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            full_prompt = self._render_prompt(query, context_docs, template_path)
        except FileNotFoundError as e:
            print(e)
            return "ì˜¤ë¥˜: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        try:
            messages = []
            if self.default_persona:
                messages.append({"role": "system", "content": self.default_persona})
            messages.append({"role": "user", "content": full_prompt})

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=cast(Any, messages),
                temperature=0.2,
                max_tokens=self.max_tokens,
            )

            return response.choices[0].message.content or "ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ì˜¤ë¥˜: ëª¨ë¸ë¡œë¶€í„° ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    def _generate_with_pipeline(
        self,
        query: str,
        context_docs: List[str],
        template_path: str,
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        # ê¸°ë³¸ì ìœ¼ë¡œ default í…œí”Œë¦¿ ì‚¬ìš©
        pipeline_template = self.prompt_pipeline_config.get("default", "prompts/scientific_qa/scientific_qa_v1.jinja2")

        # íŒŒì´í”„ë¼ì¸ ì„¤ì •ì— ë”°ë¼ í…œí”Œë¦¿ ì„ íƒ
        if self.prompt_pipeline_config:
            query_type = self._classify_query_type(query)
            if query_type in self.prompt_pipeline_config:
                pipeline_template = self.prompt_pipeline_config[query_type]

        print(f"ğŸ”„ Using prompt pipeline: {pipeline_template}")
        return self._generate_single_prompt(query, context_docs, pipeline_template)

    def _classify_query_type(self, query: str) -> str:
        """ì¿¼ë¦¬ ìœ í˜•ì„ ë¶„ë¥˜í•˜ì—¬ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ (ì¤‘ì•™í™”ëœ ë¶„ì„ ëª¨ë“ˆ ì‚¬ìš©)"""
        try:
            # Use centralized QueryAnalyzer for classification
            features = self.query_analyzer.analyze_query(query)

            # Map query types to prompt pipeline categories
            query_type_mapping = {
                "what": "scientific",
                "how": "scientific",
                "why": "conversational",
                "when": "scientific",
                "where": "scientific",
                "calculate": "scientific",
                "general": "scientific"
            }

            # Get the mapped type, defaulting to scientific for unknown types
            return query_type_mapping.get(features.query_type, "scientific")

        except Exception as e:
            print(f"âš ï¸ Query classification failed, using default: {e}")
            return "scientific"