# scripts/validate_retrieval.py

import os
import sys
import logging
from typing import cast
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from ir_core.orchestration.rewriter_openai import QueryRewriter

import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf
import wandb

# ------------------------------------

# Suppress httpx INFO logs to prevent BrokenPipeError with tqdm in multi-threaded environment
logging.getLogger("httpx").setLevel(logging.WARNING)

# OmegaConfÍ∞Ä ${env:VAR_NAME} Íµ¨Î¨∏ÏùÑ Ìï¥ÏÑùÌï† Ïàò ÏûàÎèÑÎ°ù 'env' Î¶¨Ï°∏Î≤ÑÎ•º Îì±Î°ùÌï©ÎãàÎã§.
OmegaConf.register_new_resolver("env", os.getenv)


# Add the src directory to the Python path
def _add_src_to_path():
    scripts_dir = os.path.dirname(__file__)
    repo_dir = os.path.dirname(scripts_dir)
    src_dir = os.path.join(repo_dir, "src")
    if src_dir not in sys.path:
        sys.path.insert(0, src_dir)


from ir_core.utils.wandb import generate_run_name


# --- Hydra Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ Ï†ÅÏö© (Applying the Hydra Decorator) ---
# @hydra.mainÏùÄ Ïù¥ Ïä§ÌÅ¨Î¶ΩÌä∏Ïùò ÏßÑÏûÖÏ†êÏùÑ Ï†ïÏùòÌïòÎ©∞, ÏÑ§Ï†ï ÌååÏùºÏùò Í≤ΩÎ°úÎ•º ÏßÄÏ†ïÌï©ÎãàÎã§.
# Ïù¥Ï†ú Ïù¥ Ïä§ÌÅ¨Î¶ΩÌä∏Îäî 'fire' ÎåÄÏã† HydraÎ•º ÌÜµÌï¥ Ïã§ÌñâÎê©ÎãàÎã§.
@hydra.main(config_path="../../conf", config_name="settings", version_base=None)
def run(cfg: DictConfig) -> None:
    """
    Hydra ÏÑ§Ï†ïÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌïú Í≤ÄÏÉâ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÌèâÍ∞ÄÌïòÍ≥†,
    Í∑∏ Í≤∞Í≥ºÎ•º WandBÏóê Î°úÍπÖÌï©ÎãàÎã§.

    Args:
        cfg (DictConfig): HydraÏóê ÏùòÌï¥ Í¥ÄÎ¶¨ÎêòÎäî ÏÑ§Ï†ï Í∞ùÏ≤¥.
                           conf/config.yaml ÌååÏùºÍ≥º Ïª§Îß®ÎìúÎùºÏù∏ Ïò§Î≤ÑÎùºÏù¥ÎìúÎ•º ÌÜµÌï¥ Ï±ÑÏõåÏßëÎãàÎã§.
    """
    _add_src_to_path()

    # ÌïÑÏöîÌïú Î™®ÎìàÎì§ÏùÑ ÏßÄÏó∞ ÏûÑÌè¨Ìä∏Ìï©ÎãàÎã§.
    from ir_core.config import settings
    from ir_core.generation import get_generator
    from ir_core.orchestration.pipeline import RAGPipeline
    from ir_core.utils import read_jsonl
    from ir_core.evaluation.core import mean_average_precision, average_precision
    from ir_core.analysis.core import RetrievalAnalyzer
    from ir_core.utils.wandb_logger import WandbAnalysisLogger

    # --- WandB Ï¥àÍ∏∞Ìôî (WandB Initialization) ---
    # OmegaConf.set_structÎ•º ÏÇ¨Ïö©ÌïòÏó¨ cfg Í∞ùÏ≤¥Î•º ÏûÑÏãúÎ°ú ÏàòÏ†ï Í∞ÄÎä•ÌïòÍ≤å ÎßåÎì≠ÎãàÎã§.
    OmegaConf.set_struct(cfg, False)
    # 'validate' Ïú†ÌòïÏóê ÎßûÎäî Ïã§Ìñâ Ïù¥Î¶Ñ Ï†ëÎëêÏÇ¨Î•º ÏÑ§Ï†ïÌï©ÎãàÎã§.
    cfg.wandb.run_name_prefix = cfg.wandb.run_name.validate
    run_name = generate_run_name(cfg)
    OmegaConf.set_struct(cfg, True)  # Îã§Ïãú ÏùΩÍ∏∞ Ï†ÑÏö©ÏúºÎ°ú Î≥ÄÍ≤Ω

    wandb.init(
        project=cfg.wandb.project,
        entity=cfg.wandb.entity,
        name=run_name,
        config=cast(dict, OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)),
    )

    # ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú Î°úÍπÖ (Log Config File Path)
    if hasattr(cfg.wandb, "log_config_path") and cfg.wandb.log_config_path:
        try:
            # HydraÍ∞Ä Ï†ÄÏû•Ìïú ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°úÎ•º Í∞ÄÏ†∏ÏòµÎãàÎã§
            config_path = HydraConfig.get().runtime.output_dir
            config_file = os.path.join(config_path, ".hydra", "config.yaml")
            if os.path.exists(config_file):
                print(f"Merged Config File: {config_file}")
                wandb.log({"config_file_path": config_file})
            else:
                print(f"Config file not found at expected location: {config_file}")
        except Exception as e:
            print(f"Could not determine config file path: {e}")

    print("--- Í≤ÄÏ¶ù Ïã§Ìñâ ÏãúÏûë (Starting Validation Run) ---")
    print(f"ÏÇ¨Ïö©Ìï† Í≤ÄÏ¶ù ÌååÏùº: {cfg.data.validation_path}")
    print(f"Ï†ÅÏö©Îêú ÏÑ§Ï†ï:\n{OmegaConf.to_yaml(cfg)}")

    # --- ÏÑ§Ï†ï Ïò§Î≤ÑÎùºÏù¥Îìú (Overriding Settings) ---
    # Hydra ÏÑ§Ï†ïÏùÑ Í∏∞Ï°¥Ïùò Ï†ÑÏó≠ settings Í∞ùÏ≤¥Ïóê Î∞òÏòÅÌï©ÎãàÎã§.
    # Ïù¥Îäî ÌååÏù¥ÌîÑÎùºÏù∏Ïùò Îã§Î•∏ Î∂ÄÎ∂ÑÎì§Ïù¥ ÏµúÏã† ÌååÎùºÎØ∏ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÎèÑÎ°ù Î≥¥Ïû•Ìï©ÎãàÎã§.
    settings.ALPHA = cfg.model.alpha
    settings.RERANK_K = cfg.model.rerank_k
    print(f"Alpha Í∞íÏùÑ {settings.ALPHA}(Ïúº)Î°ú Ïò§Î≤ÑÎùºÏù¥ÎìúÌï©ÎãàÎã§.")
    print(f"Rerank_k Í∞íÏùÑ {settings.RERANK_K}(Ïúº)Î°ú Ïò§Î≤ÑÎùºÏù¥ÎìúÌï©ÎãàÎã§.")

    # RAG ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.
    # ÏÑ§Ï†ïÏóêÏÑú ÏßÄÏ†ïÎêú Í≤ΩÎ°úÏùò ÎèÑÍµ¨ ÏÑ§Î™Ö ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏùΩÏñ¥ÏòµÎãàÎã§.
    try:
        with open(cfg.prompts.tool_description, "r", encoding="utf-8") as f:
            tool_desc = f.read()
    except FileNotFoundError:
        print(
            f"Ïò§Î•ò: '{cfg.prompts.tool_description}'ÏóêÏÑú ÎèÑÍµ¨ ÏÑ§Î™Ö ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
        )
        return

    # 1. QueryRewriter Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
    from ir_core.generation import get_query_rewriter

    query_rewriter = get_query_rewriter(cfg)    # 2. RAG ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ï¥àÍ∏∞ÌôîÌï† Îïå rewriter Ïù∏Ïä§ÌÑ¥Ïä§Î•º Ï†ÑÎã¨Ìï©ÎãàÎã§.
    generator = get_generator(cfg)
    pipeline = RAGPipeline(
        generator=generator,
        model_name=cfg.pipeline.tool_calling_model,  # Pass model_name for query enhancement
        query_rewriter=query_rewriter,
        tool_prompt_description=tool_desc,
        tool_calling_model=cfg.pipeline.tool_calling_model,
    )

    # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Î•º ÏùΩÏñ¥ÏòµÎãàÎã§.
    try:
        validation_data = list(read_jsonl(cfg.data.validation_path))
    except FileNotFoundError:
        print(f"Ïò§Î•ò: '{cfg.data.validation_path}'ÏóêÏÑú Í≤ÄÏ¶ù ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        return

    # --- ÏÉòÌîå Ï†úÌïú Î°úÏßÅ (Sample Limiting Logic) ---
    # cfg.limit Í∞íÏù¥ ÏÑ§Ï†ïÎêú Í≤ΩÏö∞, Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ìï¥Îãπ ÌÅ¨Í∏∞ÎßåÌÅºÎßå ÏÇ¨Ïö©Ìï©ÎãàÎã§.
    if cfg.limit:
        print(f"Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ {cfg.limit}Í∞úÏùò ÏÉòÌîåÎ°ú Ï†úÌïúÌï©ÎãàÎã§.")
        validation_data = validation_data[: cfg.limit]

    # Debug mode: Log prompts and answers for first few queries
    debug_mode = getattr(cfg, 'debug', False)
    if debug_mode:
        debug_limit = getattr(cfg, 'debug_limit', 3)
        print(f"üêõ Debug mode enabled - will log prompts and answers for first {debug_limit} queries")

    # === ANALYSIS FRAMEWORK INTEGRATION ===
    # The new analysis framework will handle all metrics collection and logging

    # === NEW ANALYSIS FRAMEWORK INTEGRATION ===
    # The new analysis framework handles all metrics collection and analysis internally

    # Prepare data for the new analysis framework
    queries_data = []
    retrieval_results_data = []

    # Use parallel processing for retrieval if enabled
    if cfg.analysis.enable_parallel and len(validation_data) > 1:
        max_workers = cfg.analysis.max_workers
        if max_workers is None:
            # Auto-determine workers: use min(sample_size, reasonable_max)
            max_workers = min(len(validation_data), 4)
        print(f"üîÑ Processing {len(validation_data)} queries using {max_workers} parallel workers...")

        def process_single_query(item, idx=0):
            """Process a single query for parallel execution."""
            debug_limit = getattr(cfg, 'debug_limit', 3)  # Define in function scope
            query = item.get("msg", [{}])[0].get("content")
            ground_truth_id = item.get("ground_truth_doc_id")

            if not query or not ground_truth_id:
                return None, None

            query_data = {"msg": [{"content": query}], "ground_truth_doc_id": ground_truth_id}

            # Get retrieval results for this query
            try:
                retrieval_output = pipeline.run_retrieval_only(query)
                if retrieval_output and isinstance(retrieval_output, list) and len(retrieval_output) > 0:
                    retrieval_result = retrieval_output[0]
                    if not isinstance(retrieval_result, dict):
                        print(f"Warning: Expected dict, got {type(retrieval_result)} for query '{query}'")
                        retrieval_result = {"docs": []}
                else:
                    retrieval_result = {"docs": []}

                # Debug mode: Log full pipeline for first few queries
                if debug_mode and idx < debug_limit:
                    print(f"\nüêõ DEBUG Query {idx + 1}: {query}")
                    try:
                        full_answer = pipeline.run(query)
                        print(f"üêõ DEBUG Answer: {full_answer[:200]}..." if len(full_answer) > 200 else f"üêõ DEBUG Answer: {full_answer}")

                        # Log retrieved context
                        docs = retrieval_result.get("docs", [])
                        if docs:
                            print(f"üêõ DEBUG Retrieved {len(docs)} documents:")
                            for i, doc in enumerate(docs[:3]):  # Show first 3 docs
                                content_preview = doc.get("content", "")[:100]
                                score = doc.get("score", 0)
                                print(f"  Doc {i+1} (score: {score:.3f}): {content_preview}...")
                        else:
                            print("üêõ DEBUG No documents retrieved")
                    except Exception as e:
                        print(f"üêõ DEBUG Error in full pipeline: {e}")

                return query_data, retrieval_result
            except Exception as e:
                print(f"Error processing query '{query}': {e}")
                return query_data, {"docs": []}

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_single_query, item, idx) for idx, item in enumerate(validation_data)]

            for future in tqdm(as_completed(futures), total=len(validation_data), desc="Validating Queries"):
                query_data, retrieval_result = future.result()
                if query_data is not None:
                    queries_data.append(query_data)
                    retrieval_results_data.append(retrieval_result)
    else:
        # Sequential processing (original logic)
        for item in tqdm(validation_data, desc="Validating Queries"):
            query = item.get("msg", [{}])[0].get("content")
            ground_truth_id = item.get("ground_truth_doc_id")

            if not query or not ground_truth_id:
                continue

            queries_data.append(
                {"msg": [{"content": query}], "ground_truth_doc_id": ground_truth_id}
            )

            # Get retrieval results for this query
            retrieval_output = pipeline.run_retrieval_only(query)
            retrieval_results_data.append(
                retrieval_output[0] if retrieval_output else {"docs": []}
            )

    # Initialize the new analysis framework
    analyzer = RetrievalAnalyzer(cfg)
    wandb_logger = WandbAnalysisLogger()

    # Perform comprehensive analysis
    analysis_result = analyzer.analyze_batch(
        queries=queries_data, retrieval_results=retrieval_results_data
    )

    # Log results using the enhanced Wandb logger
    wandb_logger.log_analysis_result(result=analysis_result)

    # Update run name with analysis results
    if wandb.run is not None:
        original_name = wandb.run.name
        updated_name = f"{original_name}-MAP_{analysis_result.map_score:.3f}"
        wandb.run.name = updated_name
        print(f"WandB Ïã§Ìñâ Ïù¥Î¶Ñ ÏóÖÎç∞Ïù¥Ìä∏: {original_name} -> {updated_name}")

    print("\n--- Í≤ÄÏ¶ù ÏôÑÎ£å (Validation Complete) ---")
    print(f"Í≤ÄÏ¶ùÎêú ÏøºÎ¶¨ Ïàò: {analysis_result.total_queries}")
    print(f"MAP Score: {analysis_result.map_score:.4f}")
    print(f"Retrieval Success Rate: {analysis_result.retrieval_success_rate:.1%}")
    print(f"Rewrite Rate: {analysis_result.rewrite_rate:.1%}")

    # Phase 4: Enhanced Error Analysis Output
    print("\n--- Phase 4: Enhanced Error Analysis ---")

    if analysis_result.query_understanding_failures:
        print("üîç Query Understanding Failures:")
        for error_type, count in analysis_result.query_understanding_failures.items():
            if count > 0:
                print(f"  ‚Ä¢ {error_type}: {count} queries")

    if analysis_result.retrieval_failures:
        print("üìä Retrieval Failures:")
        for error_type, count in analysis_result.retrieval_failures.items():
            if count > 0:
                print(f"  ‚Ä¢ {error_type}: {count} queries")

    if analysis_result.system_failures:
        print("‚ö†Ô∏è  System Failures:")
        for error_type, count in analysis_result.system_failures.items():
            if count > 0:
                print(f"  ‚Ä¢ {error_type}: {count} queries")

    if analysis_result.domain_error_rates:
        print("üåç Domain-Specific Error Rates:")
        for domain, rate in analysis_result.domain_error_rates.items():
            print(f"  ‚Ä¢ {domain}: {rate:.1%} error rate")

    if analysis_result.error_patterns.get("query_length_correlation"):
        corr = analysis_result.error_patterns["query_length_correlation"]
        print(f"üìà Query Length vs Success Correlation: {corr:.3f}")

    print("---------------------------")
    if analysis_result.recommendations:
        print("üìã Recommendations:")
        for rec in analysis_result.recommendations:
            print(f"  ‚Ä¢ {rec}")
    print("---------------------------")
    if analysis_result.error_recommendations:
        print("üîß Enhanced Error Analysis Recommendations:")
        for rec in analysis_result.error_recommendations:
            print(f"  ‚Ä¢ {rec}")
    print("---------------------------")

    if wandb.run is not None:
        print(f"WandB Ïã§Ìñâ URL: {wandb.run.url}")
    wandb.finish()


if __name__ == "__main__":
    run()
