#!/usr/bin/env python3
"""
Scientific Terms Extraction using Local Ollama Models

Processes keywords_per_src.json to extract Korean scientific terms
using locally available Ollama models (qwen2:7b or llama3.1:8b).

Usage:
    uv run python scripts/data/extract_scientific_terms.py --input_file outputs/reports/data_profile/latest/keywords_per_src.json --model qwen2:7b
"""

import json
import requests
import time
from pathlib import Path
from typing import List, Dict, Any, Set
import fire


def query_ollama(prompt: str, model: str = "qwen2:7b", max_retries: int = 3) -> str:
    """Query local Ollama model with retry logic."""
    url = "http://localhost:11434/api/generate"

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.1,  # Low temperature for consistent extraction
            "top_p": 0.9,
            "num_predict": 1000
        }
    }

    for attempt in range(max_retries):
        try:
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            return result.get("response", "").strip()
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
            else:
                raise
    return ""


def create_extraction_prompt(keywords_chunk: List[Dict[str, Any]], chunk_id: int) -> str:
    """Create a prompt for scientific term extraction."""

    # Convert keywords to simple list format for the prompt
    terms_text = []
    for src, terms in keywords_chunk:
        if isinstance(terms, list):
            for term_info in terms[:10]:  # Limit to top 10 per source
                if isinstance(term_info, dict):
                    term = term_info.get("term", "")
                    if term:
                        terms_text.append(term)

    terms_sample = "\n".join(terms_text[:50])  # Limit prompt size

    prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ê³¼í•™ ìš©ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í‚¤ì›Œë“œ ëª©ë¡ì—ì„œ **ê³¼í•™ì  ìš©ì–´ë§Œ** ì¶”ì¶œí•´ ì£¼ì„¸ìš”.

í¬í•¨í•  ìš©ì–´:
- ë¬¼ë¦¬í•™: ì›ì, ë¶„ì, ì—ë„ˆì§€, í˜, ìš´ë™, ì „ì, ì–‘ì„±ì, íŒŒë™ ë“±
- í™”í•™: í™”í•©ë¬¼, ë°˜ì‘, ì›ì†Œ, ì‚°, ì—¼ê¸°, ì´ì˜¨, ê²°í•© ë“±
- ìƒë¬¼í•™: ì„¸í¬, ìœ ì „ì, ë‹¨ë°±ì§ˆ, DNA, RNA, ì§„í™”, ëŒ€ì‚¬ ë“±
- ì˜í•™: ì§ˆë³‘, ì¹˜ë£Œ, ì§„ë‹¨, ì•½ë¬¼, í•´ë¶€í•™ ìš©ì–´ ë“±
- ì²œë¬¸í•™: ë³„, í–‰ì„±, ì€í•˜, ìš°ì£¼, ë¸”ë™í™€ ë“±
- ì§€ì§ˆí•™: ì•”ì„, ê´‘ë¬¼, ì§€ì¸µ, í™”ì‚°, ì§€ì§„ ë“±
- ìˆ˜í•™: ë°©ì •ì‹, í•¨ìˆ˜, ì ë¶„, í™•ë¥ , í†µê³„ ë“±

ì œì™¸í•  ìš©ì–´:
- ì¼ë°˜ ëª…ì‚¬ (ì‚¬ëŒ, ì‹œê°„, ì¥ì†Œ ë“±)
- ë™ì‚¬ (í•˜ë‹¤, ë˜ë‹¤, ìˆë‹¤ ë“±)
- í˜•ìš©ì‚¬ (ì¢‹ì€, ë‚˜ìœ, í° ë“±)
- ì¡°ì‚¬/ì–´ë¯¸ (ì˜, ì„, ë¥¼ ë“±)
- ì¼ë°˜ì ì¸ ë‹¨ì–´

í‚¤ì›Œë“œ ëª©ë¡ (ì²­í¬ {chunk_id}):
{terms_sample}

ê³¼í•™ ìš©ì–´ë§Œ í•œ ì¤„ì— í•˜ë‚˜ì”© ë‚˜ì—´í•´ ì£¼ì„¸ìš”:"""

    return prompt


def extract_terms_from_response(response: str) -> List[str]:
    """Extract scientific terms from model response."""
    lines = response.split('\n')
    terms = []

    for line in lines:
        line = line.strip()
        # Skip empty lines, numbers, or lines that look like explanations
        if not line or line.isdigit() or 'í¬í•¨' in line or 'ì œì™¸' in line:
            continue

        # Remove common prefixes/bullets
        line = line.lstrip('- ').lstrip('â€¢ ').lstrip('* ')

        # Basic filtering
        if len(line) > 1 and len(line) < 20:  # Reasonable term length
            terms.append(line)

    return terms


def chunk_keywords(keywords_data: Dict[str, List[Dict]], chunk_size: int = 5) -> List[List]:
    """Split keywords into manageable chunks for processing."""
    items = list(keywords_data.items())
    chunks = []

    for i in range(0, len(items), chunk_size):
        chunk = items[i:i + chunk_size]
        chunks.append(chunk)

    return chunks


def extract_scientific_terms(
    input_file: str = "outputs/reports/data_profile/latest/keywords_per_src.json",
    output_file: str = "outputs/reports/data_profile/latest/scientific_terms_extracted.json",
    model: str = "qwen2:7b",
    chunk_size: int = 5,
) -> Dict[str, Any]:
    """Extract scientific terms from keywords using local Ollama model."""

    print(f"ğŸ”¬ Extracting scientific terms using {model}")
    print(f"ğŸ“ Input: {input_file}")
    print(f"ğŸ“ Output: {output_file}")

    # Load keywords data
    input_path = Path(input_file)
    if not input_path.exists():
        raise FileNotFoundError(f"Keywords file not found: {input_file}")

    with open(input_path, 'r', encoding='utf-8') as f:
        keywords_data = json.load(f)

    print(f"ğŸ“Š Loaded keywords from {len(keywords_data)} sources")

    # Process in chunks
    chunks = chunk_keywords(keywords_data, chunk_size)
    all_scientific_terms = set()
    chunk_results = []

    print(f"ğŸ”„ Processing {len(chunks)} chunks...")

    for i, chunk in enumerate(chunks):
        print(f"â³ Processing chunk {i+1}/{len(chunks)}...")

        try:
            prompt = create_extraction_prompt(chunk, i+1)
            response = query_ollama(prompt, model)

            if response:
                terms = extract_terms_from_response(response)
                chunk_results.append({
                    "chunk_id": i+1,
                    "sources": [src for src, _ in chunk],
                    "extracted_terms": terms,
                    "term_count": len(terms)
                })
                all_scientific_terms.update(terms)
                print(f"âœ… Extracted {len(terms)} terms from chunk {i+1}")
            else:
                print(f"âš ï¸ No response for chunk {i+1}")

        except Exception as e:
            print(f"âŒ Error processing chunk {i+1}: {e}")
            continue

        # Small delay to be nice to Ollama
        time.sleep(1)

    # Prepare results
    results = {
        "model_used": model,
        "total_chunks": len(chunks),
        "total_scientific_terms": len(all_scientific_terms),
        "scientific_terms": sorted(list(all_scientific_terms)),
        "chunk_results": chunk_results,
        "extraction_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }

    # Save results
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… Extraction complete!")
    print(f"ğŸ“Š Total scientific terms extracted: {len(all_scientific_terms)}")
    print(f"ğŸ’¾ Results saved to: {output_file}")

    # Preview some terms
    if all_scientific_terms:
        preview = sorted(list(all_scientific_terms))[:20]
        print(f"ğŸ” Sample terms: {', '.join(preview)}")

    return results


if __name__ == "__main__":
    fire.Fire(extract_scientific_terms)