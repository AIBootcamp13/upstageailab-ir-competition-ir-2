#!/usr/bin/env python3
"""
Ollama Integration Demo for Scientific QA System

This script demonstrates how to integrate Ollama models into the existing
Scientific QA pipeline for cost-effective, local LLM usage.
"""

import sys
import os
import time
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.ir_core.utils.ollama_client import (
    OllamaClient,
    rewrite_query_ollama,
    generate_answer_ollama,
    generate_validation_queries_ollama,
    benchmark_ollama_model
)


def demo_query_rewriting():
    """Demonstrate query rewriting with Ollama."""
    print("ğŸ”„ Query Rewriting Demo")
    print("=" * 50)

    client = OllamaClient(model="llama3.1:8b")

    test_queries = [
        "íƒœì–‘ê³„ì˜ ë‚˜ì´ë¥¼ ì•Œì•„ë‚´ëŠ” ë° ì–´ë–¤ ë°©ë²•ë“¤ì´ ì‚¬ìš©ë˜ê³  ìˆì„ê¹Œìš”?",
        "ì›ìì˜ êµ¬ì„± ìš”ì†Œì™€ ì›ìì˜ í™”í•™ ë°˜ì‘ì—ì„œì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ê°€ê¸ˆë¥˜ ì•Œ ë‚´ë¶€ì—ì„œ ë°œê²¬ë˜ëŠ” ë‚œë°±ì€ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ” ê±´ê°€ìš”?",
        "ë¹…ë±… ì´ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]

    print("Original â†’ Rewritten")
    print("-" * 50)

    for query in test_queries:
        start_time = time.time()
        rewritten = rewrite_query_ollama(query, client)
        elapsed = time.time() - start_time

        print(f"Original: {query}")
        print(f"Rewritten: {rewritten}")
        print(".2f")
        print()


def demo_answer_generation():
    """Demonstrate answer generation with Ollama."""
    print("ğŸ¤– Answer Generation Demo")
    print("=" * 50)

    client = OllamaClient(model="llama3.1:8b")

    # Sample query and context
    query = "ì›ìì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    context_docs = [
        "ì›ìëŠ” ë¬¼ì§ˆì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ, ì–‘ì„±ì, ì¤‘ì„±ì, ì „ìë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì–‘ì„±ìì™€ ì¤‘ì„±ìëŠ” ì›ìí•µì— ìœ„ì¹˜í•˜ë©°, ì „ìëŠ” ì›ìí•µ ì£¼ìœ„ë¥¼ ê³µì „í•©ë‹ˆë‹¤.",
        "ì›ì ë²ˆí˜¸ëŠ” ì›ìí•µì— ìˆëŠ” ì–‘ì„±ìì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê° ì›ì†Œì˜ íŠ¹ì§•ì„ ê²°ì •í•©ë‹ˆë‹¤.",
        "ì „ì ë°°ì¹˜ëŠ” ì›ìì˜ í™”í•™ì  ì„±ì§ˆì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤."
    ]

    print(f"Query: {query}")
    print(f"Context documents: {len(context_docs)}")
    print()

    start_time = time.time()
    answer = generate_answer_ollama(query, context_docs, client)
    elapsed = time.time() - start_time

    print("Generated Answer:")
    print(answer)
    print(".2f")


def demo_validation_query_generation():
    """Demonstrate validation query generation with Ollama."""
    print("ğŸ“ Validation Query Generation Demo")
    print("=" * 50)

    client = OllamaClient(model="llama3.1:8b")

    domains = {
        "physics": "ë¬¼ë¦¬í•™ (í˜, ì—ë„ˆì§€, ìš´ë™, ì›ì, ì…ì ë“±)",
        "chemistry": "í™”í•™ (í™”í•©ë¬¼, ë°˜ì‘, ì›ì†Œ, ì‚°, ì—¼ê¸° ë“±)",
        "biology": "ìƒë¬¼í•™ (ì„¸í¬, ìœ ì „ì, ë‹¨ë°±ì§ˆ, ìƒëª…, ì§„í™” ë“±)"
    }

    for domain, description in domains.items():
        print(f"\nğŸ”¬ Generating queries for {domain}")
        print(f"Description: {description}")

        start_time = time.time()
        queries = generate_validation_queries_ollama(domain, description, 3, client)
        elapsed = time.time() - start_time

        print("Generated queries:")
        for i, query in enumerate(queries, 1):
            print(f"  {i}. {query}")
        print(".2f")


def demo_performance_benchmark():
    """Demonstrate performance benchmarking."""
    print("âš¡ Performance Benchmark Demo")
    print("=" * 50)

    client = OllamaClient(model="llama3.1:8b")

    test_prompts = [
        "ì›ìì˜ êµ¬ì¡°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "í™”í•™ ë°˜ì‘ì—ì„œ ì´‰ë§¤ì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "DNA ë³µì œ ê³¼ì •ì€ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ë‚˜ìš”?",
        "ë¹…ë±… ì´ë¡ ì˜ ì£¼ìš” ì¦ê±°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    ]

    print("Benchmarking Ollama model performance...")
    print(f"Model: {client.default_model}")
    print(f"Test prompts: {len(test_prompts)}")
    print()

    results = benchmark_ollama_model(client, test_prompts, num_runs=2)

    print("ğŸ“Š Benchmark Results:")
    print(".1%")
    print()

    print("Per-prompt results:")
    for result in results["benchmark_results"]:
        print(f"Prompt: {result['prompt']}")
        print(".1%")
        print(".2f")
        print(".1f")
        print()


def demo_integration_comparison():
    """Compare OpenAI vs Ollama performance."""
    print("ğŸ”„ Integration Comparison: OpenAI vs Ollama")
    print("=" * 50)

    client = OllamaClient(model="llama3.1:8b")

    test_query = "íƒœì–‘ê³„ì˜ ë‚˜ì´ë¥¼ ì•Œì•„ë‚´ëŠ” ë° ì–´ë–¤ ë°©ë²•ë“¤ì´ ì‚¬ìš©ë˜ê³  ìˆì„ê¹Œìš”?"

    print(f"Test Query: {test_query}")
    print()

    # Ollama performance
    print("ğŸ¦™ Ollama Performance:")
    start_time = time.time()
    ollama_result = rewrite_query_ollama(test_query, client)
    ollama_time = time.time() - start_time

    print(f"Rewritten: {ollama_result}")
    print(".2f")
    print("Cost: $0.00 (local)")
    print()

    # OpenAI comparison (estimated)
    print("ğŸ¤– OpenAI GPT-3.5-turbo Comparison:")
    print("Estimated time: 1-3 seconds")
    print("Estimated cost: $0.001-0.003 per request")
    print("Privacy: Data sent to OpenAI servers")
    print()

    print("ğŸ’° Cost Savings:")
    print("- Ollama: Completely free (one-time model download)")
    print("- OpenAI: ~$0.002 per request Ã— 1000 requests = $2.00")
    print("- Monthly savings: Significant for high-volume usage")


def main():
    """Run all Ollama integration demos."""
    print("ğŸš€ Ollama Integration Demo for Scientific QA System")
    print("=" * 60)
    print("This demo shows how to leverage local Ollama models for:")
    print("â€¢ Query rewriting")
    print("â€¢ Answer generation")
    print("â€¢ Validation data creation")
    print("â€¢ Performance benchmarking")
    print()

    try:
        # Check Ollama availability
        client = OllamaClient()
        if not client.check_health():
            print("âŒ Ollama server not available. Please start Ollama first:")
            print("   ollama serve")
            return

        models = client.list_models()
        if not models:
            print("âŒ No models available. Please pull a model first:")
            print("   ollama pull llama3.1:8b")
            return

        print(f"âœ… Ollama ready with models: {[m['name'] for m in models]}")
        print()

        # Run demos
        demo_query_rewriting()
        print("\n" + "="*60 + "\n")

        demo_answer_generation()
        print("\n" + "="*60 + "\n")

        demo_validation_query_generation()
        print("\n" + "="*60 + "\n")

        demo_performance_benchmark()
        print("\n" + "="*60 + "\n")

        demo_integration_comparison()

        print("\n" + "="*60)
        print("ğŸ‰ Ollama Integration Demo Complete!")
        print("\nNext Steps:")
        print("1. Integrate Ollama into your validation scripts")
        print("2. Replace OpenAI calls with Ollama for cost savings")
        print("3. Experiment with different models (llama3.1:70b, codellama, etc.)")
        print("4. Set up automated model updates and performance monitoring")

    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        print("Make sure Ollama is running and accessible")


if __name__ == "__main__":
    main()
