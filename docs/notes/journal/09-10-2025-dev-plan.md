
### 2. 개선 계획 (Refactor Plan)

제안된 대화 분석 프레임워크는 매우 타당하며, 현재 프로젝트가 겪는 핵심 문제를 정확히 해결할 수 있는 올바른 방향입니다. 이를 바탕으로 기존 코드베이스에 적용할 수 있는 구체적인 실행 계획을 단계별로 제안합니다.

#### **타당성 평가 (Feasibility Assessment)**

* **제안된 대화 분석 프레임워크는 매우 실현 가능합니다.** 현재 프로젝트의 모듈화된 구조 덕분에, 대화 맥락을 처리하는 새로운 모듈을 기존 파이프라인에 '주입(inject)'하는 형태로 비교적 쉽게 통합할 수 있습니다.

#### **구체적인 실행 계획 (Concrete Implementation Plan)**

**Phase 1: 기반 개선 (Foundational Improvements)**

* [ ] **설정 중앙화 (Centralize Configuration)**
    * `conf/config.yaml`에 실험 관련 모든 하이퍼파라미터(모델 이름, `top_k`, `bm25_k` 등)를 추가합니다.
    * Hydra 또는 Pydantic 설정을 확장하여 코드 내 하드코딩된 값들을 이 설정 파일에서 읽어오도록 리팩토링합니다. 이는 재현성과 실험의 용이성을 높입니다.
* [ ] **W&B 연동 (Integrate W&B)**
    * `validate_retrieval.py`와 `evaluate.py` 스크립트에 W&B 연동 코드를 추가합니다.
    * `wandb.init()`으로 실험을 초기화하고, `config`에 `alpha`, `rerank_k`, `embedding_model` 등 주요 하이퍼파라미터를 저장합니다.
    * 실행이 끝난 후 `wandb.log()`를 사용하여 최종 MAP 점수와 같은 평가지표를 기록합니다. 이를 통해 어떤 파라미터 조합이 최고의 성능을 내는지 쉽게 추적할 수 있습니다.

**Phase 2: 핵심 로직 개선 - 대화 이해 (Core Logic Enhancement - Conversation Understanding)**

* [ ] **`RAGPipeline` 입력 구조 변경**
    * `run` 및 `run_retrieval_only` 메서드가 단일 `query` 문자열 대신, `eval.jsonl`의 `msg`와 같은 대화 기록(메시지 객체의 리스트)을 입력으로 받도록 수정합니다.
* [ ] **"질의 재구성(Query Rephrasing)" 모듈 구현 (가장 중요한 단계)**
    * 대화 기록을 입력으로 받아 LLM을 호출하는 새로운 함수 또는 클래스를 만듭니다.
    * **목표:** LLM에게 "이 대화의 맥락을 요약하고, 만약 과학적 질문이 포함되어 있다면 독립적으로 이해 가능한 단일 검색 질의(standalone query)를 한국어로 생성하라. 과학적 질문이 아니라면 빈 문자열('')을 출력하라." 와 같은 명확한 역할을 부여합니다.
    * 이 단계의 출력이 `scientific_search` 도구에 전달될 `standalone_query`가 됩니다.
* [ ] **질의 재구성을 위한 프롬프트 개발**
    * `prompts/` 디렉토리에 이 "질의 재구성" 작업을 위한 새로운 Jinja2 템플릿 파일을 생성합니다. 이 프롬프트는 대화 기록을 변수로 받아 처리하도록 설계되어야 합니다.
* [ ] **파이프라인 통합**
    * `RAGPipeline` 내에서 `scientific_search` 도구를 호출하기 *전에* 이 새로운 "질의 재구성" 모듈을 먼저 실행하도록 파이프라인의 흐름을 변경합니다.
    * 재구성된 질의가 비어있지 않을 때만 `scientific_search`를 호출하고, 비어있다면 잡담으로 간주하여 검색 없이 바로 답변 생성 단계로 넘어갑니다.

**Phase 3: 평가 및 시각화 (Evaluation & Visualization)**

* [ ] **Streamlit 대시보드 개발**
    * `eval.jsonl` 파일과 생성된 `submission.jsonl` 파일을 나란히 보여주는 UI를 만듭니다.
    * `eval_id`를 기준으로 특정 대화와 그에 대한 모델의 출력(standalone_query, topk 문서, 생성된 답변)을 쉽게 비교/분석할 수 있는 기능을 구현합니다. 이는 정성적 분석(qualitative analysis)에 매우 유용합니다.
    * 대시보드에 MAP 같은 주요 성능 지표를 표시합니다.
* [ ] **탐색적 데이터 분석 (EDA) 수행**
    * `analyze_data.py` 스크립트를 활용하여 `documents.jsonl`의 문서 길이 분포를 분석하고, 토큰화 시 `max_length` 설정의 적절성을 검토합니다.
    * `eval.jsonl`의 대화 길이나 턴(turn) 수 분포를 분석하여 모델이 처리해야 할 대화의 복잡도를 파악합니다.