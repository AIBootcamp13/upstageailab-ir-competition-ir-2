### **프로젝트 발전을 위한 도구 및 기능 평가**

#### **1. 핵심 경쟁력 확보를 위한 필수 도구 (Must-Haves)**

이 도구들은 대회의 독특한 평가 방식을 직접적으로 공략하여 높은 MAP 점수를 획득하는 데 필수적입니다.

* **고급 쿼리 분석 및 라우팅 (Advanced Query Analysis & Routing)**
    * **평가:** 제공된 자료의 핵심은 "과학 상식 질문이 아닌 경우, 문서를 추출하지 않아야 높은 점수를 받는다"는 것입니다. 현재 귀하의 `RAGPipeline`은 LLM의 도구 호출(Tool Calling) 기능을 사용하여 이 로직을 이미 완벽하게 구현하고 있습니다. 이는 프로젝트의 가장 큰 강점 중 하나입니다.
    * **필요한 도구:** 별도의 외부 도구가 필요 없습니다. 현재의 **`RAGPipeline`과 `ToolDispatcher` 아키텍처**가 바로 이 기능을 수행하는 핵심 "도구"입니다. 앞으로의 과제는 LLM이 `scientific_search` 도구를 언제, 어떻게 호출할지 더 정교하게 만들도록 프롬프트를 튜닝하는 것입니다.

* **체계적인 프롬프트 엔지니어링 프레임워크 (Systematic Prompt Engineering Framework)**
    * **평가:** 대회 목표 달성과 향후 챗봇 애플리케이션 개발 모두에서 프롬프트의 성능이 절대적으로 중요합니다. 현재 `Jinja2`를 사용해 프롬프트를 외부 파일로 분리한 것은 매우 훌륭한 첫걸음입니다.
    * **필요한 도구/전략:**
        1.  **프롬프트 버전 관리:** `prompts` 디렉토리 내에서 `scientific_qa_v2.jinja2`, `conversational_v2.jinja2`와 같이 프롬프트 변경 사항을 버전별로 관리하는 체계를 확립해야 합니다.
        2.  **A/B 테스팅:** 두 가지 버전의 프롬프트를 사용하여 동일한 평가 데이터셋으로 결과를 비교하고, 어떤 프롬프트가 더 나은 MAP 점수를 내는지 정량적으로 측정하는 프로세스가 필요합니다.

#### **2. 경쟁 우위를 위한 고급 도구 (Should-Haves)**

이 도구들은 제공된 자료에서 제안된 고급 기법들을 구현하여, 다른 참가자들과 차별화되는 성능을 만들어낼 수 있습니다.

* **합성 데이터 생성 파이프라인 (Synthetic Data Generation Pipeline)**
    * **평가:** 참고 자료에서는 Question-Answer 쌍 데이터가 없는 점을 지적하며 **Inverse Cloze Task (ICT)**와 **Question Generation (QG)**을 대안으로 제시합니다. 이는 단순히 좋은 아이디어를 넘어, 고성능 검색 모델(예: ColBERT)을 미세 조정(fine-tuning)하기 위한 핵심 전제 조건입니다.
    * **필요한 도구/기능:**
        1.  **ICT 스크립트:** `documents.jsonl` 파일을 입력받아 `(의사 쿼리, 관련 문서)` 쌍을 생성하는 새로운 스크립트(예: `scripts/generate_ict_pairs.py`)가 필요합니다.
        2.  **QG 스크립트:** `documents.jsonl`의 내용을 바탕으로 자연스러운 질문을 생성하는 스크립트(예: `scripts/generate_qg_pairs.py`)가 필요합니다. 이를 위해 자료에서 언급된 `Pororo` 라이브러리를 사용하거나, 현재 시스템에 이미 통합된 OpenAI/Ollama LLM을 활용할 수 있습니다.

* **LLM 기반 평가 스위트 (LLM-based Evaluation Suite)**
    * **평가:** MAP 점수는 검색 성능만 측정하지만, 최종 생성된 답변의 품질은 측정하지 못합니다. 참고 자료에서 제안된 'LLM을 활용한 자동 채점'은 프로젝트의 실제 성능을 파악하고 향후 LibreChat 애플리케이션으로 발전시키는 데 매우 유용합니다.
    * **필요한 도구/기능:**
        1.  **"LLM-as-Judge" 스크립트:** 참고 자료의 프롬프트와 로직을 구현한 새로운 평가 스크립트(예: `scripts/evaluate_generation.py`)가 필요합니다. 이 스크립트는 `(질문, 검색된 문서, 생성된 답변)`을 LLM에 보내 정성적인 점수(관련성, 명확성 등)를 받아옵니다. 이는 A/B 테스트 시 어떤 프롬프트나 모델이 정말로 "더 나은" 답변을 만드는지 판단하는 데 결정적인 역할을 합니다.

#### **3. 미처 고려하지 못했을 수 있는 정보**

위의 내용 외에도, 프로젝트의 규모가 커지고 실험이 복잡해짐에 따라 다음과 같은 도구들이 매우 유용해질 것입니다.

* **실험 추적 및 관리 도구 (Experiment Tracking & Management)**
    * **평가:** 다양한 프롬프트 버전, `alpha` 값, 임베딩 모델, 합성 데이터셋을 사용해 실험을 진행하면, 어떤 조건에서 어떤 결과가 나왔는지 추적하기가 매우 어려워집니다.
    * **추천 도구:** **Weights & Biases (W&B)** 또는 **MLflow**. `pyproject.toml`에 이미 `wandb`가 포함되어 있으므로 이를 활용하는 것이 가장 효율적입니다. `evaluate.py` 스크립트를 수정하여 각 실행의 하이퍼파라미터(예: 프롬프트 버전, alpha값)와 결과(MAP 점수)를 W&B에 기록하도록 만들면, 모든 실험 결과를 체계적으로 비교하고 분석할 수 있습니다.

* **LLM 호출 캐싱 (LLM Call Caching)**
    * **평가:** 현재 Redis는 임베딩 캐시에만 사용되고 있습니다. 하지만 `evaluate.py`나 "LLM-as-Judge" 스크립트를 여러 번 실행하면, 동일한 입력으로 LLM API를 반복적으로 호출하게 되어 불필요한 비용과 시간이 발생합니다.
    * **추천 도구/전략:** Redis를 활용하여 **LLM API 호출 결과를 캐싱**하는 로직을 `RAGPipeline`과 LLM 기반 평가 스크립트에 추가해야 합니다. `(모델명, 프롬프트)`를 키로 사용하여 API 응답을 저장하면, 동일한 요청에 대해서는 API를 호출하지 않고 캐시에서 즉시 결과를 가져올 수 있습니다.

* **고급 설정 관리 (Advanced Configuration Management)**
    * **평가:** 실험이 많아지면 `.env` 파일만으로는 설정을 관리하기가 복잡해집니다.
    * **추천 도구:** **Hydra**. `pyproject.toml`에 이미 `hydra-core`가 포함되어 있습니다. 이를 활용하여 `conf/` 디렉토리 내에 `experiment_A.yaml`, `experiment_B.yaml`과 같은 설정 파일을 만들어 실험별로 다른 파라미터(모델, 프롬프트 경로 등)를 쉽게 관리하고 실행 시점에 동적으로 조합할 수 있습니다.

이 평가가 프로젝트의 다음 단계를 계획하는 데 도움이 되길 바랍니다. 현재 아키텍처는 이러한 고급 기능들을 통합하기에 매우 좋은 구조입니다.