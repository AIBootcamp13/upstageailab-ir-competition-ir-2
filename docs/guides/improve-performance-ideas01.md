

1.  **메타데이터를 활용한 검색 성능 향상**
2.  **향상된 검증 데이터셋 구축**
3.  **생성된 답변의 신뢰성(Faithfulness) 테스트**

-----

### **1. 메타데이터를 활용한 검색 성능 향상**

현재 당신은 문서의 `content` 전체를 임베딩하여 벡터 검색을 수행하고 있습니다. 여기에 구조화된 메타데이터를 추가하면, 단순 벡터 검색의 한계를 보완하여 검색의 정확도를 비약적으로 높일 수 있습니다.

#### **어떤 메타데이터를 추가해야 하는가?**

핵심은 LLM을 사용하여 각 문서로부터 **요약(Summaries)**, **키워드/엔티티(Keywords/Entities)**, 그리고 \*\*가상 질문(Hypothetical Questions)\*\*을 미리 추출하여 인덱스에 함께 저장하는 것입니다.

#### **어떻게 작동하는가?**

1.  **전처리 단계 (Offline)**: 모든 문서를 인덱싱하기 전에, 각 문서에 대해 LLM을 사용하여 아래와 같은 메타데이터를 생성합니다.
2.  **인덱싱**: 원본 `content`와 함께 생성된 메타데이터를 Elasticsearch의 각기 다른 필드에 저장합니다.
3.  **검색 단계 (Online)**: 사용자의 질문이 들어오면, 질문의 특성에 맞게 여러 필드(벡터, 키워드, 요약 등)를 동시에 검색하여 가장 관련성 높은 문서를 찾습니다.

#### **구체적인 적용 예시**

**샘플 문서:**
`{"docid": "bbcd99d8...", "content": "회절격자는 빛의 간섭에 의존합니다..."}`

**1. LLM으로 메타데이터 생성:**

  * **요약 (Summary)**:

    > "이 문서는 회절격자가 빛의 파동성과 간섭 현상을 이용하여 빛의 파장, 주파수 등 다양한 특성을 측정하는 중요한 장치임을 설명합니다."

      * **이점**: 길고 장황한 원문 대신, 핵심 내용만 담은 요약을 임베딩하면 더 밀도 높고 정확한 벡터 표현을 얻을 수 있습니다.

  * **키워드 & 엔티티 (Keywords & Entities)**:

    > `["회절격자", "빛의 간섭", "파동성", "회절 패턴", "파장", "주파수", "물리학"]`

      * **이점**: "회절격자의 원리는?"과 같이 특정 용어가 포함된 질문에 대해 키워드 검색(BM25)이 벡터 검색보다 더 빠르고 정확하게 작동할 수 있습니다.

  * **가상 질문 (Hypothetical Questions)**:

    >   * `회절격자는 어떤 물리적 원리를 이용하는가?`
    >   * `빛의 간섭 현상을 사용하는 과학 장비는 무엇이 있는가?`
    >   * `회절격자로 빛의 어떤 특성들을 측정할 수 있는가?`

      * **이점 (매우 중요)**: 사용자의 실제 질문과 문서 내용 사이의 간극을 메워주는 가장 강력한 기법입니다. 사용자가 "빛의 특성을 재는 기계가 뭐야?"라고 물었을 때, 원문에는 없는 '기계'라는 단어 대신, 생성된 가상 질문과의 벡터 유사도를 통해 이 문서를 정확히 찾아낼 수 있습니다.

#### **실행 계획: 강화된 인덱스 구조**

Elasticsearch 인덱스의 매핑(mapping)을 다음과 같이 확장하여 이 메타데이터를 저장합니다.

```json
{
  "mappings": {
    "properties": {
      "docid": { "type": "keyword" },
      "content": { "type": "text", "analyzer": "nori" },

      "summary": { "type": "text", "analyzer": "nori" },
      "keywords": { "type": "keyword" },
      "hypothetical_questions": { "type": "text", "analyzer": "nori" },

      "embedding_summary": { "type": "dense_vector", "dims": 768 },
      "embedding_hypothetical_questions": { "type": "dense_vector", "dims": 768 }
    }
  }
}
```

이 구조에서는 `summary`와 `hypothetical_questions`의 텍스트 자체와 그에 대한 벡터 임베딩을 모두 저장하여 하이브리드 검색의 효율을 극대화합니다.

-----

### **2. 향상된 검증 데이터셋 구축**

현재의 테스트 데이터는 "질문"만 포함하고 있어, 검색이 잘 되었는지(`Retrieval`)만 평가할 수 있습니다. 생성된 답변이 얼마나 좋은지(`Generation`)는 평가할 수 없습니다.

#### **해결책: "질문-문맥-답변" 삼중주(Triplet) 데이터셋 구축**

더 정교한 평가를 위해, LLM을 사용하여 다음과 같은 형식의 데이터셋을 구축해야 합니다.

**기존 형식:**
`{"eval_id": 78, "msg": [{"role": "user", "content": "나무의 분류에 대해 조사해 보기 위한 방법은?"}]}`

**향상된 형식:**

```json
{
  "eval_id": 78,
  "query": "나무의 분류에 대해 조사해 보기 위한 방법은?",
  "ideal_context": [
    "한 학생이 다양한 종류의 나무를 조사하고 있습니다. ... 이는 생물 분류학에서 중요한 기준 중 하나입니다. ...",
    "고든 여사의 수업은 자연과학에 관한 것입니다. ... 미터 자를 사용하여 길이를 측정하는 기술을 익힐 수 있습니다."
  ],
  "ideal_answer": "나무를 분류하는 방법에는 성장 속도, 온도 범위, 그리고 잎과 꽃의 형태적 특징을 비교하는 생물 분류학적 기준을 사용하는 방법이 있습니다. 또한, 미터 자와 같은 도구를 사용하여 나뭇가지의 길이를 측정하는 것도 한 방법이 될 수 있습니다.",
  "hard_negative_context": [
    "종이를 재활용하면 환경에 여러 가지 이점을 가져오게 됩니다. ... 나무를 보존하는 데 도움이 되고..."
  ]
}
```

  * `ideal_context`: 질문에 답변하기 위해 반드시 필요한 핵심 내용을 담은 문서 조각.
  * `ideal_answer`: 위 `ideal_context`만을 근거로 생성된, 가장 이상적인 답변.
  * `hard_negative_context`: 주제는 비슷하지만(둘 다 '나무'에 대한 내용), 질문에 대한 직접적인 답변은 포함하지 않는 "함정" 문서. 리랭커가 이런 문서를 얼마나 잘 걸러내는지를 평가하는 데 사용됩니다.

#### **실행 계획: LLM을 이용한 데이터셋 생성**

`scripts/data/` 폴더에 새로운 스크립트를 만들어, LLM(예: `gpt-4o-mini`)에 기존 질문과 관련 문서를 함께 제공하고, 위 형식에 맞춰 `ideal_answer`와 `hard_negative_context`를 생성하도록 요청할 수 있습니다.

-----

### **3. 생성된 답변의 신뢰성(Faithfulness) 테스트**

RAG 시스템의 가장 큰 숙제는 LLM이 검색된 `문맥`에 근거하여 답변을 생성하는지, 아니면 사실이 아닌 내용을 지어내는지(환각, Hallucination)를 확인하는 것입니다.

#### **해결책: LLM을 심판으로 활용 (LLM-as-a-Judge)**

가장 일반적이고 효과적인 방법은, 또 다른 강력한 LLM(예: `gpt-4o-mini`)을 '심판'으로 사용하여 생성된 답변의 신뢰도를 평가하는 것입니다.

#### **실행 계획: 신뢰도 평가 프롬프트**

평가 스크립트에서, 모델이 생성한 답변이 나오면 아래와 같은 프롬프트를 사용하여 심판 LLM에게 평가를 요청합니다.

**심판 LLM에게 보낼 프롬프트 템플릿:**

```
[역할]
당신은 RAG 시스템의 답변을 평가하는 정교한 AI 심판입니다. 당신의 임무는 '생성된 답변'이 제공된 '문맥'에 얼마나 충실한지를 판단하는 것입니다. '문맥'에 언급되지 않은 내용은 사실이더라도 "신뢰할 수 없는" 것으로 간주해야 합니다.

[문맥]
{{retrieved_context}}

[생성된 답변]
{{generated_answer}}

[평가 지침]
1. '생성된 답변'의 모든 문장이 '문맥'에 의해 뒷받침되는지 확인하십시오.
2. '문맥'에 없는 정보를 포함하고 있다면 신뢰할 수 없는 답변입니다.
3. 평가 점수를 1점에서 5점까지 부여하고, 그 이유를 간략하게 설명하십시오.

[평가 형식]
{
  "faithfulness_score": (1-5 사이의 정수),
  "reasoning": "(평가 이유)"
}
```

이 세 가지 계획을 순차적으로 또는 병렬적으로 진행하면, 제한된 시간 안에 당신의 RAG 시스템을 체계적으로 개선하고 최고 수준의 성능으로 끌어올릴 수 있을 것입니다.